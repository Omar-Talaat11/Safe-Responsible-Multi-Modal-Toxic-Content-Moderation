ğŸ›¡ï¸ Multi-Modal Toxic Content Moderation
This project is focused on detecting and classifying toxic content from both text and images. It uses fine-tuned transformer models, image captioning, and a Streamlit interface to allow users to test and visualize predictions.

ğŸ“Œ Features
âœ… Classify text into multiple toxic categories using DistilBERT fine-tuned with LoRA

ğŸ–¼ï¸ Classify images by generating captions using BLIP and feeding them to the text classifier

ğŸ§  Includes a hard classifier (LLaMA Guard) to ensure safety before detailed classification

ğŸ“Š Balanced dataset through data cleaning, class merging, and augmentation with BERT

ğŸŒ Deployed as an interactive Streamlit app

ğŸ How It Works
Text Input:

Cleaned and lemmatized

Passed to LLaMA Guard (hard classifier)

If safe, passed to DistilBERT (soft classifier) for toxic category classification

Image Input:

Caption generated using BLIP

Caption goes through same text classification pipeline

ğŸ“Š Model Info
Model: DistilBERT fine-tuned with LoRA

Tokenizer: distilbert-base-uncased

Additional tools:

BLIP for image captioning

LLaMA Guard via OpenRouter for safety filtering

nlpaug for BERT-based data augmentation

ğŸ“ Notes
Handles class imbalance by merging rare classes and augmenting mid-range ones

Evaluation includes classification report and confusion matrix

Final model is saved for inference (my_model/ folder)
